{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycaret-ts-alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret.time_series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-47adb61b24e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_series\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret.time_series'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pycaret.time_series import *\n",
    "from pycaret.datasets import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pycaret\n",
      "Version: 2.3.10\n",
      "Summary: PyCaret - An open source, low-code machine learning library in Python.\n",
      "Home-page: https://github.com/pycaret/pycaret\n",
      "Author: Moez Ali\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Author-email: moez.ali@queensu.ca\n",
      "License: MIT\n",
      "Location: c:\\users\\zeesh\\anaconda3\\envs\\pycaret\\lib\\site-packages\n",
      "Requires: matplotlib, Boruta, pyLDAvis, imbalanced-learn, ipywidgets, yellowbrick, pyod, spacy, wordcloud, mlxtend, lightgbm, mlflow, scikit-plot, joblib, kmodes, pandas-profiling, numba, IPython, pandas, scipy, scikit-learn, nltk, seaborn, plotly, cufflinks, umap-learn, gensim, pyyaml, textblob\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "pip show pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import mlflow\n",
    "# import pandas as pd\n",
    "# from fbprophet import Prophet\n",
    "# from pycaret.datasets import get_data\n",
    "\n",
    "# def train_and_log_prophet(data, experiment_name='forecasting_experiment_with_Prophet'):\n",
    "#     # Start a single MLflow run for logging all models\n",
    "#     with mlflow.start_run():\n",
    "\n",
    "#         # Log experiment details\n",
    "#         mlflow.log_param(\"experiment_name\", experiment_name)\n",
    "\n",
    "#         # Loop through each time series\n",
    "#         for column in data.columns:\n",
    "#             # Prepare data for Prophet\n",
    "#             df_prophet = data[[column]].reset_index()\n",
    "#             df_prophet = df_prophet.rename(columns={'Date': 'ds', column: 'y'})\n",
    "\n",
    "#             # Create and train the Prophet model\n",
    "#             model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\n",
    "#             model.fit(df_prophet)\n",
    "\n",
    "#             # Optional: Save the model object\n",
    "#             model_filename = f\"prophet_model_{column}.pkl\"\n",
    "#             with open(model_filename, \"wb\") as f:\n",
    "#                 pickle.dump(model, f)\n",
    "\n",
    "#             # Log the model with MLflow using its name\n",
    "#             mlflow.log_param(\"column_name\", column)\n",
    "#             mlflow.log_metric(\"Prophet_MAE\", model.predict(df_prophet).yhat.abs().mean())\n",
    "#             mlflow.sklearn.log_model(model, f\"prophet_{column}\")  # Use a unique model name for logging\n",
    "\n",
    "# # ... data preparation and setup\n",
    "# data = get_data('pycaret_downloads')\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "# data.set_index('Date', inplace=True)\n",
    "\n",
    "# # Train and log Prophet models for each time series\n",
    "# train_and_log_prophet(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import mlflow\n",
    "# import pandas as pd\n",
    "# from pycaret.time_series import *\n",
    "# from pycaret.datasets import get_data\n",
    "\n",
    "# def train_and_log_model(data, experiment_name='forecasting_experiment_with_PyCaret'):\n",
    "#     \"\"\"\n",
    "#     Train AutoML forecasting model using PyCaret and log metrics and models using MLflow.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - data: DataFrame, the input dataset\n",
    "#     - experiment_name: str, the name of the MLflow experiment\n",
    "    \n",
    "#     Returns:\n",
    "#     - None\n",
    "#     \"\"\"\n",
    "#     # Set up PyCaret experiment\n",
    "#     exp = setup(data, fold=3, fh=12, session_id=123, experiment_name=experiment_name)\n",
    "\n",
    "#     # Get the best model\n",
    "#     best_model = compare_models()\n",
    "\n",
    "#     # Start a single MLflow run for logging all models\n",
    "#     with mlflow.start_run():\n",
    "\n",
    "#         # Log experiment ID\n",
    "#         mlflow.log_param(\"experiment_id\", exp.id)\n",
    "\n",
    "#         # Loop through all generated models\n",
    "#         for model_name in best_model:\n",
    "#             # Create and train the model\n",
    "#             model = create_model(model_name)\n",
    "#             trained_model = finalize_model(model)\n",
    "\n",
    "#             # Optional: Save the model object\n",
    "#             model_filename = f\"trained_model_{model_name}.pkl\"\n",
    "#             with open(model_filename, \"wb\") as f:\n",
    "#                 pickle.dump(trained_model, f)\n",
    "\n",
    "#             # Log the model with MLflow using its name\n",
    "#             mlflow.log_metric(\"MAE\", model.MAE)\n",
    "#             mlflow.log_param(\"model_type\", model.model_frame.model_class)\n",
    "#             mlflow.sklearn.log_model(trained_model, model_name)  # Use model name for logging\n",
    "\n",
    "#             # Save all available metrics from PyCaret experiment\n",
    "#             for metric_name, metric_value in model.evaluate().items():\n",
    "#                 mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "# # ... data preparation and setup\n",
    "# data = get_data('pycaret_downloads')\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "# data.set_index('Date', inplace=True)\n",
    "\n",
    "# # Train and log models\n",
    "# train_and_log_model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import mlflow\n",
    "# import pandas as pd\n",
    "# from pycaret.time_series import *\n",
    "# from pycaret.datasets import get_data\n",
    "\n",
    "# def train_and_log_model(data, experiment_name='forecasting_experiment_with_PyCaret', r2_threshold=0.7):\n",
    "#     \"\"\"\n",
    "#     Train AutoML forecasting model using PyCaret and log metrics and models using MLflow.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - data: DataFrame, the input dataset\n",
    "#     - experiment_name: str, the name of the MLflow experiment\n",
    "#     - r2_threshold: float, the threshold for R-squared\n",
    "    \n",
    "#     Returns:\n",
    "#     - None\n",
    "#     \"\"\"\n",
    "#     # Set up PyCaret experiment\n",
    "#     exp = setup(data, fold=3, fh=12, session_id=123, experiment_name=experiment_name)\n",
    "\n",
    "#     # Get the best models\n",
    "#     best_models = compare_models()\n",
    "\n",
    "#     # Start a single MLflow run for logging all models\n",
    "#     with mlflow.start_run():\n",
    "        \n",
    "#         # Log experiment details\n",
    "#         mlflow.log_param(\"experiment_name\", experiment_name)\n",
    "\n",
    "#         # Loop through all generated models\n",
    "#         for model_name in best_models:\n",
    "#             # Create and train the model\n",
    "#             model = create_model(model_name)\n",
    "            \n",
    "#             # Check if R-squared is above the threshold\n",
    "#             if model.r2 > r2_threshold:\n",
    "#                 trained_model = finalize_model(model)\n",
    "\n",
    "#                 # Optional: Save the model object\n",
    "#                 model_filename = f\"trained_model_{model_name}.pkl\"\n",
    "#                 with open(model_filename, \"wb\") as f:\n",
    "#                     pickle.dump(trained_model, f)\n",
    "\n",
    "#                 # Log the model with MLflow using its name\n",
    "#                 mlflow.log_metric(\"R2\", model.r2)\n",
    "#                 mlflow.log_param(\"model_type\", model.model_frame.model_class)\n",
    "#                 mlflow.sklearn.log_model(trained_model, model_name)  # Use model name for logging\n",
    "\n",
    "#                 # Save all available metrics from PyCaret experiment\n",
    "#                 metrics = model.evaluate()\n",
    "#                 for metric_name, metric_value in metrics.items():\n",
    "#                     mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "# # ... data preparation and setup\n",
    "# data = get_data('pycaret_downloads')\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "# data.set_index('Date', inplace=True)\n",
    "\n",
    "# # Train and log models with R-squared above 0.7\n",
    "# train_and_log_model(data, r2_threshold=0.65)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pycaret.time_series import *\n",
    "from pycaret.datasets import get_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... data preparation and setup\n",
    "data = get_data('pycaret_downloads')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming daily_data is a DataFrame with a 'date' column\n",
    "dt=data.reset_index()\n",
    "df_test= dt.copy()\n",
    "\n",
    "fig = px.line(df_test, x='Date', y='Total', markers=True)\n",
    "# Update the marker color\n",
    "fig.update_traces(marker=dict(color='red'))\n",
    "\n",
    "# Update the layout to include zooming functionality\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n",
    "                dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible=True),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret time series and init setup\n",
    "s = setup(data, fh = 12, session_id = 123)\n",
    "# s = setup(data, fh=3, session_id=123, log_experiment='mlflow', experiment_name='/Users/<username>/pycaret_downloads_experiment')\n",
    "\n",
    "# from pycaret.time_series import TSForecastingExperiment\n",
    "s1= TSForecastingExperiment()\n",
    "# s1.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = pull(best)\n",
    "# final_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_mae_models_top3 = compare_models(sort = 'R2', n_select = 3)\n",
    "# list of top 3 models by MAE\n",
    "# best_mae_models_top3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive=create_model('naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=create_model('theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_theta= tune_model(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA-2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='train_test_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='decomp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='decomp', data_kwargs={'type':'Multiplicative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='decomp', data_kwargs={'sesonal_period':24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='acf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot='pacf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot = 'diagnostics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "plot_model(best, plot = 'forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot forecast for 36 months in future\n",
    "plot_model(best, plot = 'forecast', data_kwargs = {'fh' : 36})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(estimator=tuned_theta, data_kwargs={\"fh\":36})\n",
    "# plot_model(estimator=naive, data_kwargs={\"fh\":36})\n",
    "# plot_model([theta, tuned_theta], data_kwargs={\"labels\": [\"Baseline\", \"Tuned\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot\n",
    "# plot_model(best, plot = 'insample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot\n",
    "plot_model(best, plot = 'residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot\n",
    "plot_model(best, plot = 'diagnostics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "holdout_pred = predict_model(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow.tracking\n",
    "\n",
    "# Replace with your experiment ID\n",
    "experiment_id = \"4421816471787578\"\n",
    "\n",
    "# Get all runs within the experiment\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "runs = client.search_runs(experiment_ids=experiment_id)\n",
    "\n",
    "# Create an empty dataframe\n",
    "df_runs = pd.DataFrame(columns=[\"Run ID\", \"Model Name\"])\n",
    "\n",
    "# Iterate through each run and extract model name and metrics\n",
    "for run in runs:\n",
    "    run_data = client.get_run(run.info.run_id)\n",
    "    model_name = run_data.data.params.get(\"model_name\")  # Assuming \"model_name\" is logged as a parameter\n",
    "    metrics = run_data.data.metrics  # Capture all logged metrics\n",
    "\n",
    "    # Append the run data to the dataframe\n",
    "    row_data = {\"Run ID\": run.info.run_id, \"Model Name\": model_name}\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        row_data[metric_name] = metric_value\n",
    "        \n",
    "    df_runs = df_runs.append(row_data, ignore_index=True)\n",
    "\n",
    "# Replace missing metric values with \"NA\"\n",
    "df_runs = df_runs.fillna(\"NA\")\n",
    "\n",
    "# Print the dataframe\n",
    "display(df_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (a) Getting top 1 best model run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Replace with your experiment ID\n",
    "experiment_id = \"4421816471787578\"\n",
    "\n",
    "# Specify the metric to evaluate the best run\n",
    "metric = \"rsquared\"\n",
    "\n",
    "# Fetch the best run based on the given metric\n",
    "best_run = mlflow.search_runs(\n",
    "    experiment_ids=experiment_id,\n",
    "    order_by=[f\"metric.{metric} DESC\"],\n",
    "    max_results=1\n",
    ").iloc[0]\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id=best_run.run_id\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (b) Registrattion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='PyCaret_model'\n",
    "model_path=best_run.artifact_uri\n",
    "mlflow.register_model(f\"{model_path}/model\",model_name)\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### (c) Changing stage to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "model_version = 1  # replace with the actual model version\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the best Model saved in Artifact folder in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pycaret.datasets import get_data\n",
    "#...data preparation and setup\n",
    "model_name='PyCaret_model'\n",
    "test_set = get_data('pycaret_downloads')\n",
    "test_set['Date'] = pd.to_datetime(test_set['Date'])\n",
    "test_set.drop('Total', axis=1, inplace=True)\n",
    "\n",
    "logged_model = 'runs:/344c3662bafe48389d0a0aaa834ac5ee/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
